{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7792312,"sourceType":"datasetVersion","datasetId":4561467},{"sourceId":8271070,"sourceType":"datasetVersion","datasetId":4910730}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport nltk\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:48:29.817126Z","iopub.execute_input":"2024-04-30T10:48:29.817453Z","iopub.status.idle":"2024-04-30T10:48:32.822985Z","shell.execute_reply.started":"2024-04-30T10:48:29.817428Z","shell.execute_reply":"2024-04-30T10:48:32.822145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding = 'ISO-8859-1'\ncol_names = ['Sentiment', 'Twtter_Id', 'Date', 'Flag', 'UserName', 'Tweet']\n\n#Non-Depressive Tweets\ndataset = pd.read_csv('/kaggle/input/sentimental-analysis/training.1600000.processed.noemoticon.csv', encoding=encoding, names=col_names)\ndataset = dataset[dataset['Sentiment'] == 4]\ndataset = dataset[['Tweet','Sentiment']]\ndataset['Sentiment'].replace(4, 0, inplace=True)\ndataset = dataset.sample(15000, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:48:32.825114Z","iopub.execute_input":"2024-04-30T10:48:32.825611Z","iopub.status.idle":"2024-04-30T10:48:38.841740Z","shell.execute_reply.started":"2024-04-30T10:48:32.825578Z","shell.execute_reply":"2024-04-30T10:48:38.840806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset1 = pd.read_csv(\"/kaggle/input/sentimental-analysis/Mental-Health-Twitter.csv\",encoding=encoding)\ndataset1.rename(columns={'post_text': 'Tweet', 'label': 'Sentiment'}, inplace=True)\ndataset1 = dataset1[dataset1['Sentiment'] == 1]\ndataset1 = dataset1[['Tweet','Sentiment']]\ncol_names = ['UserName','Tweet']\ndataset2 = pd.read_csv('/kaggle/input/sentimental-analysis/depressive_tweets_processed.csv', sep = '|', header = None, usecols = [4,5], nrows = 3200, names=col_names)\ndataset2['Sentiment'] = 1;\ndataset2 = dataset2[['Tweet','Sentiment']]\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:48:38.842958Z","iopub.execute_input":"2024-04-30T10:48:38.843206Z","iopub.status.idle":"2024-04-30T10:48:38.973752Z","shell.execute_reply.started":"2024-04-30T10:48:38.843186Z","shell.execute_reply":"2024-04-30T10:48:38.972827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset1 = pd.concat([dataset1,dataset2])\ndataset1 = dataset1.sample(frac = 1)\ndataset1.reset_index(drop=True, inplace=True)\ndataset = pd.concat([dataset,dataset1])\ndataset = dataset.sample(frac = 1)\ndataset.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:48:38.976041Z","iopub.execute_input":"2024-04-30T10:48:38.976394Z","iopub.status.idle":"2024-04-30T10:48:38.987831Z","shell.execute_reply.started":"2024-04-30T10:48:38.976370Z","shell.execute_reply":"2024-04-30T10:48:38.986898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = dataset.copy()\ndf.reset_index(drop=True, inplace=True)\ndf.isnull()\ndf.dropna(inplace=True)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-30T10:48:38.988940Z","iopub.execute_input":"2024-04-30T10:48:38.989189Z","iopub.status.idle":"2024-04-30T10:48:39.003144Z","shell.execute_reply.started":"2024-04-30T10:48:38.989168Z","shell.execute_reply":"2024-04-30T10:48:39.002267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-30T10:48:39.004263Z","iopub.execute_input":"2024-04-30T10:48:39.004622Z","iopub.status.idle":"2024-04-30T10:48:39.018054Z","shell.execute_reply.started":"2024-04-30T10:48:39.004584Z","shell.execute_reply":"2024-04-30T10:48:39.017136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('wordnet')\nnltk.download('stopwords')\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-30T10:48:39.019143Z","iopub.execute_input":"2024-04-30T10:48:39.019476Z","iopub.status.idle":"2024-04-30T10:48:39.172248Z","shell.execute_reply.started":"2024-04-30T10:48:39.019446Z","shell.execute_reply":"2024-04-30T10:48:39.171403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\n\n# Load the English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Lemmatization function using SpaCy\ndef lemmatize_text(text):\n    # Process the text with SpaCy\n    doc = nlp(text)\n    # Lemmatize each token and join them back into a string\n    lemmatized_tokens = [token.lemma_ for token in doc]\n    return ' '.join(lemmatized_tokens)\n\n# Apply the lemmatization function to the 'Tweet' column\n#df[\"Tweet\"] = df[\"Tweet\"].apply(lemmatize_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:48:39.173188Z","iopub.execute_input":"2024-04-30T10:48:39.173473Z","iopub.status.idle":"2024-04-30T10:48:47.886713Z","shell.execute_reply.started":"2024-04-30T10:48:39.173451Z","shell.execute_reply":"2024-04-30T10:48:47.885933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nimport pandas as pd\nimport emoji\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n                           \"you're\": \"you are\", \"you've\": \"you have\"}\n\n\ndef text_transformation(text):\n    if isinstance(text, str):\n        text = text.lower() #LOWER CASE\n        text = emoji.demojize(text)\n        text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])  \n        text = re.sub('\\[.*?\\]', '', text) #REMOVES BRACKETS AND BTW THEM\n        text = re.sub(\"\\\\W\",\" \",text) #REMOVES NON-WORDS\n        text = re.sub('https?://\\S+|www\\.\\S+', '', text) #REMOVES LINKS\n        text = re.sub('<.*?>+', '', text) #REMOVES HTML TAGS\n        text = re.sub('[%s]' % re.escape(string.punctuation), '', text) #REMOVES PUNCUTATIONS\n        text = re.sub('\\n', '', text) #REMOVES NEW LINES\n        text = re.sub('\\w*\\d\\w*', '', text)  #REMOVES ALPHA NUMERICAL  \n        text = re.sub(' +', ' ', text)  # Replace multiple spaces with a single space\n    return text\n\ndf[\"Tweet\"] = df[\"Tweet\"].apply(text_transformation)\ndf[\"Tweet\"] = df[\"Tweet\"].apply(lemmatize_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:48:47.887902Z","iopub.execute_input":"2024-04-30T10:48:47.888410Z","iopub.status.idle":"2024-04-30T10:52:02.698169Z","shell.execute_reply.started":"2024-04-30T10:48:47.888383Z","shell.execute_reply":"2024-04-30T10:52:02.697377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n\n# Define a custom list of generalized stopwords\ngeneralized_stopwords = [\n    'i','' 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n    'you', 'your', 'yours', 'yourself', 'yourselves',\n    'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n    'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n    'a', 'an', 'at','the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to',\n    'from', 'up', 'down', 'in', 'out', 'im','on', 'off', 'over', 'under', 'again',\n    'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n    'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n    'such','only', 'own', 'same', 'so', 'than', 'too','https','twitter','tweet',\n]\n\n\ndf[\"Tweet\"] = df[\"Tweet\"].apply(lambda x: \" \".join(x for x in x.split() if x not in generalized_stopwords))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:52:02.700906Z","iopub.execute_input":"2024-04-30T10:52:02.701188Z","iopub.status.idle":"2024-04-30T10:52:03.320224Z","shell.execute_reply.started":"2024-04-30T10:52:02.701165Z","shell.execute_reply":"2024-04-30T10:52:03.319371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\nword_cloud = \"\"\nfor row in df['Tweet']:\n    for word in row:\n        word_cloud += \" \".join(word)\n\nwordcloud = WordCloud(width=1000, height=500, background_color='white', min_font_size=6).generate(word_cloud)\nfig, ax = plt.subplots(figsize=(10, 5))\n# Define the rectangle patch\nrect = Rectangle((0.1, 0.1), 0.5, 0.3, linewidth=1, edgecolor='r', facecolor='none')\n# Add the rectangle patch to the plot\nax.add_patch(rect)\nplt.imshow(wordcloud)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:52:03.321354Z","iopub.execute_input":"2024-04-30T10:52:03.321658Z","iopub.status.idle":"2024-04-30T10:52:07.923761Z","shell.execute_reply.started":"2024-04-30T10:52:03.321634Z","shell.execute_reply":"2024-04-30T10:52:07.922876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install textblob\n# Lemmatization (to group similar words together)\nnltk.download(\"wordnet\")\nnltk.download(\"omw-1.4\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-30T10:52:07.924987Z","iopub.execute_input":"2024-04-30T10:52:07.925936Z","iopub.status.idle":"2024-04-30T10:52:22.807990Z","shell.execute_reply.started":"2024-04-30T10:52:07.925900Z","shell.execute_reply":"2024-04-30T10:52:22.806877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom gensim.models import Word2Vec\nimport nltk\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef performance_eval(clf, X_test, y_test):\n    y_pred = clf.predict(X_test)\n\n    print(f'Accuracy: {accuracy_score(y_test, y_pred)}\\n')\n    \n    print('   ------------ Classification Report -----------')\n    print(classification_report(y_test, y_pred))\n    \n    print('   ------------ Confusion Matrix -------------- ')\n    cm = confusion_matrix(y_test, y_pred)\n    \n    sns.set(rc={'figure.figsize':(10,6)})\n    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True,cmap = \"Blues\",fmt='d', \n            xticklabels=['Negative', 'Positive'],\n            yticklabels=['Negative', 'Positive'])\n    plt.text(0.5, 0.4, f'True Negative ', ha='center', va='center',color='white')\n    plt.text(0.5, 1.4, f'False Positive ', ha='center', va='center')\n    plt.text(1.5, 0.4, f'False Negative ', ha='center', va='center')\n    plt.text(1.5, 1.4, f'True Positive ', ha='center', va='center',color='white')\n\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-30T10:52:22.809668Z","iopub.execute_input":"2024-04-30T10:52:22.811719Z","iopub.status.idle":"2024-04-30T10:52:39.384447Z","shell.execute_reply.started":"2024-04-30T10:52:22.811690Z","shell.execute_reply":"2024-04-30T10:52:39.383625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#    1. LOGISTIC REGRESSION","metadata":{}},{"cell_type":"markdown","source":"###         • Using Bag Of Words (CountVectorizer)","metadata":{}},{"cell_type":"code","source":"X = df['Tweet']\ny = df['Sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ncount_vectorizer = CountVectorizer()\nX_train = count_vectorizer.fit_transform(X_train)\nX_test = count_vectorizer.transform(X_test)\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nprint(\"Logistic Regression (BOW)\\n\")\nperformance_eval(lr,X_test,y_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:52:39.385803Z","iopub.execute_input":"2024-04-30T10:52:39.386840Z","iopub.status.idle":"2024-04-30T10:52:41.684784Z","shell.execute_reply.started":"2024-04-30T10:52:39.386804Z","shell.execute_reply":"2024-04-30T10:52:41.683920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###         • Using TF-IDF","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ntfidf_vectorizer = TfidfVectorizer()\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\nlr = LogisticRegression()\nlr.fit(X_train_tfidf, y_train)\nprint(\"Logistic Regression (TF-IDF)\\n\")\nperformance_eval(lr,X_test_tfidf,y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:52:41.686144Z","iopub.execute_input":"2024-04-30T10:52:41.686599Z","iopub.status.idle":"2024-04-30T10:52:43.315147Z","shell.execute_reply.started":"2024-04-30T10:52:41.686567Z","shell.execute_reply":"2024-04-30T10:52:43.314294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#    2. DECISION TREE","metadata":{}},{"cell_type":"markdown","source":"###         • Using Bag Of Words (CountVectorizer)","metadata":{}},{"cell_type":"code","source":"X = df['Tweet']\ny = df['Sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ncount_vectorizer = CountVectorizer()\nX_train = count_vectorizer.fit_transform(X_train)\nX_test = count_vectorizer.transform(X_test)\ndt_classifier = DecisionTreeClassifier()\ndt_classifier.fit(X_train, y_train)\nprint(\"Decision Tree (BOW)\\n\")\nperformance_eval(dt_classifier,X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:53:12.158677Z","iopub.execute_input":"2024-04-30T10:53:12.159017Z","iopub.status.idle":"2024-04-30T10:53:21.157974Z","shell.execute_reply.started":"2024-04-30T10:53:12.158993Z","shell.execute_reply":"2024-04-30T10:53:21.157087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###         • Using TF-IDF","metadata":{}},{"cell_type":"code","source":"X = df['Tweet']\ny = df['Sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntfidf_vectorizer = TfidfVectorizer()\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\ndt_classifier = DecisionTreeClassifier()\ndt_classifier.fit(X_train_tfidf, y_train)\nprint(\"Decision Tree (TF-IDF)\\n\")\nperformance_eval(dt_classifier,X_test_tfidf,y_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:53:21.159911Z","iopub.execute_input":"2024-04-30T10:53:21.160183Z","iopub.status.idle":"2024-04-30T10:53:31.240091Z","shell.execute_reply.started":"2024-04-30T10:53:21.160160Z","shell.execute_reply":"2024-04-30T10:53:31.239172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#    3. RANDOM FOREST","metadata":{}},{"cell_type":"code","source":"X = df['Tweet']\ny = df['Sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ncount_vectorizer = CountVectorizer()\nX_train = count_vectorizer.fit_transform(X_train)\nX_test = count_vectorizer.transform(X_test)\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(X_train, y_train)\nprint(\"Random Forest (BOW)\\n\")\nperformance_eval(rf_classifier,X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:53:31.241227Z","iopub.execute_input":"2024-04-30T10:53:31.241606Z","iopub.status.idle":"2024-04-30T10:54:13.973328Z","shell.execute_reply.started":"2024-04-30T10:53:31.241575Z","shell.execute_reply":"2024-04-30T10:54:13.972384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###         • Using TF-IDF","metadata":{}},{"cell_type":"code","source":"X = df['Tweet']\ny = df['Sentiment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntfidf_vectorizer = TfidfVectorizer()\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(X_train_tfidf, y_train)\nprint(\"Random Forest (TF-IDF)\\n\")\nperformance_eval(rf_classifier,X_test_tfidf,y_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:54:13.975415Z","iopub.execute_input":"2024-04-30T10:54:13.975699Z","iopub.status.idle":"2024-04-30T10:54:50.483701Z","shell.execute_reply.started":"2024-04-30T10:54:13.975677Z","shell.execute_reply":"2024-04-30T10:54:50.482783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tensorflow\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:54:50.485023Z","iopub.execute_input":"2024-04-30T10:54:50.485410Z","iopub.status.idle":"2024-04-30T10:55:05.704963Z","shell.execute_reply.started":"2024-04-30T10:54:50.485377Z","shell.execute_reply":"2024-04-30T10:55:05.703734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom sklearn.model_selection import train_test_split\n\n# Assuming df is your DataFrame with columns 'Tweet' and 'Sentiment'\nX = df['Tweet']\ny = df['Sentiment']\n\n# Tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X)\nX_seq = tokenizer.texts_to_sequences(X)\n\n# Pad sequences to have consistent length\nX_padded = pad_sequences(X_seq)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n\n# Build the LSTM model\nembedding_dim = 50  # You can adjust this based on your data\nmax_length = X_padded.shape[1]\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_length))\nmodel.add(LSTM(100))  # You can adjust the number of LSTM units\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'LSTM Model Accuracy: {accuracy * 100:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:55:05.706753Z","iopub.execute_input":"2024-04-30T10:55:05.707167Z","iopub.status.idle":"2024-04-30T10:56:43.302844Z","shell.execute_reply.started":"2024-04-30T10:55:05.707128Z","shell.execute_reply":"2024-04-30T10:56:43.301812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install deap\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-30T10:56:43.304274Z","iopub.execute_input":"2024-04-30T10:56:43.304704Z","iopub.status.idle":"2024-04-30T10:56:55.802202Z","shell.execute_reply.started":"2024-04-30T10:56:43.304669Z","shell.execute_reply":"2024-04-30T10:56:55.800986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom deap import base, creator, tools, algorithms\nimport random\n\n# Assume df is your dataframe with columns 'Tweet' and 'Sentiment'\n\n# Feature extraction using Bag of Words\nvectorizer = CountVectorizer(max_features=1000)  # Adjust max_features as needed\nX = vectorizer.fit_transform(df['Tweet']).toarray()\ny = df['Sentiment']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define genetic algorithm parameters\npopulation_size = 1000\nnum_generations = 10\ncrossover_prob = 0.7\nmutation_prob = 0.6\n\n# Create a fitness function\ndef evaluate(individual, X_train, y_train, X_test, y_test):\n    selected_features = [bool(bit) for bit in individual]\n    clf = DecisionTreeClassifier(random_state=42)\n    clf.fit(X_train[:, selected_features], y_train)\n    y_pred = clf.predict(X_test[:, selected_features])\n    accuracy = accuracy_score(y_test, y_pred)\n    return accuracy,\n\n# Define genetic algorithm parameters\ncreator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\ncreator.create(\"Individual\", list, fitness=creator.FitnessMax)\n\ntoolbox = base.Toolbox()\ntoolbox.register(\"attr_bool\", random.randint, 0, 1)\ntoolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=len(X[0]))\ntoolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\ntoolbox.register(\"evaluate\", evaluate, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\ntoolbox.register(\"mate\", tools.cxTwoPoint)\ntoolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\ntoolbox.register(\"select\", tools.selTournament, tournsize=3)\n\n# Initialize population\npopulation = toolbox.population(n=population_size)\n\n# Run the genetic algorithm\nalgorithms.eaSimple(population, toolbox, cxpb=crossover_prob, mutpb=mutation_prob, ngen=num_generations, stats=None, halloffame=None)\n\n# Get the best individual\nbest_individual = tools.selBest(population, k=1)[0]\nselected_features = [bool(bit) for bit in best_individual]\n\n# Use the selected features in your model\nfinal_X_train = X_train[:, selected_features]\nfinal_X_test = X_test[:, selected_features]\n\n# Train your model using the selected features\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(final_X_train, y_train)\n\n# Evaluate the performance\nperformance_eval(clf, final_X_test, y_test)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:56:55.803941Z","iopub.execute_input":"2024-04-30T10:56:55.804258Z","iopub.status.idle":"2024-04-30T11:00:26.291160Z","shell.execute_reply.started":"2024-04-30T10:56:55.804231Z","shell.execute_reply":"2024-04-30T11:00:26.290159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEST\n'''\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom deap import base, creator, tools, algorithms\nimport random\nfrom tqdm import tqdm  # Import tqdm for progress tracking\n\n# Assume df is your dataframe with columns 'Tweet' and 'Sentiment'\n\n# Feature extraction using Bag of Words\nvectorizer = CountVectorizer(max_features=1000)  # Adjust max_features as needed\nX = vectorizer.fit_transform(df['Tweet']).toarray()\ny = df['Sentiment']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define genetic algorithm parameters\npopulation_size = 1000\nnum_generations = 1\ncrossover_prob = 0.7\nmutation_prob = 0.6\n\n# Create a fitness function\ndef evaluate(individual, X_train, y_train, X_test, y_test):\n    selected_features = [bool(bit) for bit in individual]\n    clf = DecisionTreeClassifier(random_state=42)\n    clf.fit(X_train[:, selected_features], y_train)\n    y_pred = clf.predict(X_test[:, selected_features])\n    accuracy = accuracy_score(y_test, y_pred)\n    return accuracy,\n\n# Define genetic algorithm parameters\ncreator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\ncreator.create(\"Individual\", list, fitness=creator.FitnessMax)\n\ntoolbox = base.Toolbox()\ntoolbox.register(\"attr_bool\", random.randint, 0, 1)\ntoolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=len(X[0]))\ntoolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\ntoolbox.register(\"evaluate\", evaluate, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\ntoolbox.register(\"mate\", tools.cxTwoPoint)\ntoolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\ntoolbox.register(\"select\", tools.selTournament, tournsize=3)\n\n# Initialize population\npopulation = toolbox.population(n=population_size)\n\n# Run the genetic algorithm with tqdm progress tracking\nfor generation in tqdm(range(num_generations), desc='Genetic Algorithm Progress'):\n    # Evaluate the population fitness\n    fitness_values = list(map(toolbox.evaluate, population))\n    \n    # Update the fitness values for each individual\n    for ind, fit in zip(population, fitness_values):\n        ind.fitness.values = fit\n        \n    # Apply selection, crossover, and mutation\n    offspring = algorithms.varAnd(population, toolbox, cxpb=crossover_prob, mutpb=mutation_prob)\n    \n    # Evaluate the new individuals\n    fits = toolbox.map(toolbox.evaluate, offspring)\n    for fit, ind in zip(fits, offspring):\n        ind.fitness.values = fit\n        \n    # Replace the current population with the offspring\n    population[:] = offspring\n\n# Get the best individual\nbest_individual = tools.selBest(population, k=1)[0]\nselected_features = [bool(bit) for bit in best_individual]\n\n# Use the selected features in your model\nfinal_X_train = X_train[:, selected_features]\nfinal_X_test = X_test[:, selected_features]\n\n# Train your model using the selected features\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(final_X_train, y_train)\n\n# Evaluate the performance\nperformance_eval(clf, final_X_test, y_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:00:26.292805Z","iopub.execute_input":"2024-04-30T11:00:26.293446Z","iopub.status.idle":"2024-04-30T11:04:52.271034Z","shell.execute_reply.started":"2024-04-30T11:00:26.293411Z","shell.execute_reply":"2024-04-30T11:04:52.269184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import random\nfrom deap import base, creator, tools, algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:04:52.272115Z","iopub.status.idle":"2024-04-30T11:04:52.272694Z","shell.execute_reply.started":"2024-04-30T11:04:52.272407Z","shell.execute_reply":"2024-04-30T11:04:52.272430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# Assuming df is your DataFrame with 'Tweet' and 'Label' columns\nX = df['Tweet']\ny = df['Sentiment']\n\n# Split your dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:04:52.274528Z","iopub.status.idle":"2024-04-30T11:04:52.274981Z","shell.execute_reply.started":"2024-04-30T11:04:52.274754Z","shell.execute_reply":"2024-04-30T11:04:52.274772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''def evaluate_features(individual, X_train, X_test, y_train, y_test):\n    # Select features based on the chromosome\n    selected_features = [feature for feature, selected in zip(X_train.columns, individual) if selected]\n    \n    # Create a bag-of-words representation using CountVectorizer\n    vectorizer = CountVectorizer()\n    X_train_transformed = vectorizer.fit_transform(X_train[selected_features])\n    X_test_transformed = vectorizer.transform(X_test[selected_features])\n\n    # Train a random forest classifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X_train_transformed, y_train)\n\n    # Evaluate accuracy on the test set\n    y_pred = clf.predict(X_test_transformed)\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:04:52.276417Z","iopub.status.idle":"2024-04-30T11:04:52.276897Z","shell.execute_reply.started":"2024-04-30T11:04:52.276654Z","shell.execute_reply":"2024-04-30T11:04:52.276674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\ncreator.create(\"Individual\", list, fitness=creator.FitnessMax)\n\ntoolbox = base.Toolbox()\ntoolbox.register(\"attr_bool\", random.randint, 0, 1)\ntoolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=len(X_train.columns))\ntoolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n\ntoolbox.register(\"evaluate\", evaluate_features, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\ntoolbox.register(\"mate\", tools.cxTwoPoint)\ntoolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\ntoolbox.register(\"select\", tools.selTournament, tournsize=3)\n\n# Create an initial population\npopulation = toolbox.population(n=50)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:04:52.278526Z","iopub.status.idle":"2024-04-30T11:04:52.278976Z","shell.execute_reply.started":"2024-04-30T11:04:52.278751Z","shell.execute_reply":"2024-04-30T11:04:52.278769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# Number of generations\nn_gen = 10\n#X_test = X_test.to_frame()\n\n# Run the genetic algorithm\nalgorithms.eaMuPlusLambda(population, toolbox, mu=50, lambda_=200, cxpb=0.7, mutpb=0.2, ngen=n_gen, stats=None, halloffame=None, verbose=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:04:52.280645Z","iopub.status.idle":"2024-04-30T11:04:52.280987Z","shell.execute_reply.started":"2024-04-30T11:04:52.280825Z","shell.execute_reply":"2024-04-30T11:04:52.280839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pip install tqdm\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:04:52.282914Z","iopub.status.idle":"2024-04-30T11:04:52.283324Z","shell.execute_reply.started":"2024-04-30T11:04:52.283116Z","shell.execute_reply":"2024-04-30T11:04:52.283132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Define your dataset and labels\nX = df[\"Tweet\"].values\ny = df[\"Sentiment\"].values\n\n# Use CountVectorizer for text to convert it into a bag-of-words representation\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(X)\n\n# Define the parameters\npopulation_size = 20\nnum_generations = 10\ncrossover_prob = 0.9\nmutation_prob = 0.1\n\n# Define the classifier\nclf = DecisionTreeClassifier()\n\n# Function to initialize the population\ndef initialize_population():\n    return np.random.choice([0, 1], size=(population_size, X.shape[1]))\n\n\n# Function to evaluate the fitness of each individual\ndef evaluate_fitness(population):\n    accuracies = []\n    for features in population:\n        selected_features = np.where(features == 1)[0]\n        \n        # Use selected features for training\n        X_train, X_test, y_train, y_test = train_test_split(X[:, selected_features], y, test_size=0.2, random_state=42)\n        \n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        \n        accuracy = accuracy_score(y_test, y_pred)\n        accuracies.append(accuracy)\n    \n    return np.array(accuracies)\n\n# Function to perform crossover\ndef crossover(parent1, parent2):\n    crossover_point = np.random.randint(0, len(parent1))\n    child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n    child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n    return child1, child2\n\n# Function to perform mutation\ndef mutate(individual):\n    mutation_points = np.random.rand(len(individual)) < mutation_prob\n    individual[mutation_points] = 1 - individual[mutation_points]\n    return individual\n\n# Main genetic algorithm loop\npopulation = initialize_population()\nfor generation in range(num_generations):\n    fitness_values = evaluate_fitness(population)\n    \n    # Select top individuals based on fitness\n    selected_indices = np.argsort(fitness_values)[::-1][:int(population_size * 0.2)]\n    selected_population = population[selected_indices]\n    \n    # Create new population through crossover and mutation\n    new_population = []\n    for _ in range(population_size // 2):\n        parent1, parent2 = selected_population[np.random.choice(len(selected_population), size=2, replace=False)]\n        child1, child2 = crossover(parent1, parent2)\n        child1 = mutate(child1)\n        child2 = mutate(child2)\n        new_population.extend([child1, child2])\n    \n    population = np.array(new_population)\n\n# Select the best individual from the final population\nbest_individual = population[np.argmax(evaluate_fitness(population))]\n\n# Use the selected features for training the final model\nselected_features = np.where(best_individual == 1)[0]\nX_train_final, X_test_final, y_train_final, y_test_final = train_test_split(X[:, selected_features], y, test_size=0.2, random_state=42)\nfinal_model = DecisionTreeClassifier()\nfinal_model.fit(X_train_final, y_train_final)\n\n# Evaluate the final model\ny_pred_final = final_model.predict(X_test_final)\naccuracy_final = accuracy_score(y_test_final, y_pred_final)\nprint(\"Final Accuracy:\", accuracy_final)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T11:04:52.284657Z","iopub.status.idle":"2024-04-30T11:04:52.284995Z","shell.execute_reply.started":"2024-04-30T11:04:52.284832Z","shell.execute_reply":"2024-04-30T11:04:52.284846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport random\n\n# Define your dataset and labels\nX = df[\"Tweet\"].values\ny = df[\"Sentiment\"].values\n\n# Use CountVectorizer for text to convert it into a bag-of-words representation\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(X)\n\n# Define the parameters\npopulation_size = 20\nnum_generations = 10\ncrossover_prob = 0.9\nmutation_prob = 0.1\n\n# Function to initialize the population\ndef initialize_population():\n    return np.random.choice([0, 1], size=(population_size, X.shape[1]))\n\n# Function to evaluate the fitness of each individual\ndef evaluate_fitness(population):\n    accuracies = []\n    for features in population:\n        selected_features = np.where(features == 1)[0]\n        \n        # Use selected features for training\n        X_train, X_test, y_train, y_test = train_test_split(X[:, selected_features], y, test_size=0.2, random_state=42)\n        \n        clf = DecisionTreeClassifier(random_state=42)\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        \n        accuracy = accuracy_score(y_test, y_pred)\n        accuracies.append(accuracy)\n    \n    return np.array(accuracies)\n\n# Function to perform crossover\ndef crossover(parent1, parent2):\n    crossover_point = np.random.randint(0, len(parent1))\n    child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n    child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n    return child1, child2\n\n# Function to perform mutation\ndef mutate(individual):\n    mutation_points = np.random.rand(len(individual)) < mutation_prob\n    individual[mutation_points] = 1 - individual[mutation_points]\n    return individual\n\n# Main genetic algorithm loop\npopulation = initialize_population()\nfor generation in range(num_generations):\n    fitness_values = evaluate_fitness(population)\n    \n    # Select top individuals based on fitness\n    selected_indices = np.argsort(fitness_values)[::-1][:int(population_size * 0.2)]\n    selected_population = population[selected_indices]\n    \n    # Create new population through crossover and mutation\n    new_population = []\n    for _ in range(population_size // 2):\n        parent1, parent2 = selected_population[np.random.choice(len(selected_population), size=2, replace=False)]\n        child1, child2 = crossover(parent1, parent2)\n        child1 = mutate(child1)\n        child2 = mutate(child2)\n        new_population.extend([child1, child2])\n    \n    population = np.array(new_population)\n\n# Select the best individual from the final population\nbest_individual = population[np.argmax(evaluate_fitness(population))]\n\n# Use the selected features for training the final model\nselected_features = np.where(best_individual == 1)[0]\nX_train_final, X_test_final, y_train_final, y_test_final = train_test_split(X[:, selected_features], y, test_size=0.2, random_state=42)\n\n# Try a Random Forest as the final model\nfinal_model = RandomForestClassifier(random_state=42)\nfinal_model.fit(X_train_final, y_train_final)\n\n# Evaluate the final model using cross-validation\ncv_accuracy = np.mean(cross_val_score(final_model, X_test_final, y_test_final, cv=5))\nprint(\"Cross-Validation Accuracy:\", cv_accuracy)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Word2Vec\n\n#BiLSTM\n\n#BERT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport random\n\n# Assume df is your dataframe with columns 'Tweet' and 'Sentiment'\n\n# Feature extraction using TF-IDF\nvectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\nX = vectorizer.fit_transform(df['Tweet']).toarray()\ny = df['Sentiment']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the parameters\npopulation_size = 20\nnum_generations = 10\ncrossover_prob = 0.9\nmutation_prob = 0.1\n\n# Function to initialize the population\ndef initialize_population():\n    return np.random.choice([0, 1], size=(population_size, X.shape[1]))\n\n# Function to evaluate the fitness of each individual\ndef evaluate_fitness(population):\n    accuracies = []\n    for features in population:\n        selected_features = np.where(features == 1)[0]\n        \n        # Use selected features for training\n        X_train, X_test, y_train, y_test = train_test_split(X[:, selected_features], y, test_size=0.2, random_state=42)\n        \n        clf = DecisionTreeClassifier(random_state=42)\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        \n        accuracy = accuracy_score(y_test, y_pred)\n        accuracies.append(accuracy)\n    \n    return np.array(accuracies)\n\n# Function to perform crossover\ndef crossover(parent1, parent2):\n    crossover_point = np.random.randint(0, len(parent1))\n    child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n    child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n    return child1, child2\n\n# Function to perform mutation\ndef mutate(individual):\n    mutation_points = np.random.rand(len(individual)) < mutation_prob\n    individual[mutation_points] = 1 - individual[mutation_points]\n    return individual\n\n# Main genetic algorithm loop\npopulation = initialize_population()\nfor generation in range(num_generations):\n    fitness_values = evaluate_fitness(population)\n    \n    # Select top individuals based on fitness\n    selected_indices = np.argsort(fitness_values)[::-1][:int(population_size * 0.2)]\n    selected_population = population[selected_indices]\n    \n    # Create new population through crossover and mutation\n    new_population = []\n    for _ in range(population_size // 2):\n        parent1, parent2 = selected_population[np.random.choice(len(selected_population), size=2, replace=False)]\n        child1, child2 = crossover(parent1, parent2)\n        child1 = mutate(child1)\n        child2 = mutate(child2)\n        new_population.extend([child1, child2])\n    \n    population = np.array(new_population)\n\n# Select the best individual from the final population\nbest_individual = population[np.argmax(evaluate_fitness(population))]\n\n# Use the selected features for training the final model\nselected_features = np.where(best_individual == 1)[0]\nX_train_final, X_test_final, y_train_final, y_test_final = train_test_split(X[:, selected_features], y, test_size=0.2, random_state=42)\n\n# Try a Random Forest as the final model\nfinal_model = RandomForestClassifier(random_state=42)\nfinal_model.fit(X_train_final, y_train_final)\n\n# Evaluate the final model using cross-validation\ncv_accuracy = np.mean(cross_val_score(final_model, X_test_final, y_test_final, cv=5))\nprint(\"Cross-Validation Accuracy:\", cv_accuracy)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import cross_val_score\n\n# Assume df is your dataframe with columns 'Tweet' and 'Sentiment'\n\n# Tokenize the tweets into words\ntokenized_tweets = df['Tweet'].apply(lambda x: x.split())\n\n# Train a Word2Vec model\nword2vec_model = Word2Vec(sentences=tokenized_tweets, vector_size=100, window=5, min_count=1, workers=4)\n\n# Feature extraction using Word2Vec\ndef vectorize_tweet(tweet):\n    vector = np.zeros(word2vec_model.vector_size)\n    count = 0\n    for word in tweet:\n        if word in word2vec_model.wv:\n            vector += word2vec_model.wv[word]\n            count += 1\n    return vector / count if count != 0 else vector\n\nX = np.vstack(tokenized_tweets.apply(vectorize_tweet))\ny = df['Sentiment']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the parameters\npopulation_size = 20\nnum_generations = 10\ncrossover_prob = 0.9\nmutation_prob = 0.1\n\n# Function to initialize the population\ndef initialize_population():\n    return np.random.choice([0, 1], size=(population_size, X.shape[1]))\n\n# Function to evaluate the fitness of each individual\ndef evaluate_fitness(population):\n    accuracies = []\n    for features in population:\n        selected_features = np.where(features == 1)[0]\n        \n        # Use selected features for training\n        X_train_selected, X_test_selected = X_train[:, selected_features], X_test[:, selected_features]\n        \n        clf = DecisionTreeClassifier(random_state=42)\n        clf.fit(X_train_selected, y_train)\n        y_pred = clf.predict(X_test_selected)\n        \n        accuracy = accuracy_score(y_test, y_pred)\n        accuracies.append(accuracy)\n    \n    return np.array(accuracies)\n\n# Function to perform crossover\ndef crossover(parent1, parent2):\n    crossover_point = np.random.randint(0, len(parent1))\n    child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n    child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n    return child1, child2\n\n# Function to perform mutation\ndef mutate(individual):\n    mutation_points = np.random.rand(len(individual)) < mutation_prob\n    individual[mutation_points] = 1 - individual[mutation_points]\n    return individual\n\n# Main genetic algorithm loop\npopulation = initialize_population()\nfor generation in range(num_generations):\n    fitness_values = evaluate_fitness(population)\n    \n    # Select top individuals based on fitness\n    selected_indices = np.argsort(fitness_values)[::-1][:int(population_size * 0.2)]\n    selected_population = population[selected_indices]\n    \n    # Create new population through crossover and mutation\n    new_population = []\n    for _ in range(population_size // 2):\n        parent1, parent2 = selected_population[np.random.choice(len(selected_population), size=2, replace=False)]\n        child1, child2 = crossover(parent1, parent2)\n        child1 = mutate(child1)\n        child2 = mutate(child2)\n        new_population.extend([child1, child2])\n    \n    population = np.array(new_population)\n\n# Select the best individual from the final population\nbest_individual = population[np.argmax(evaluate_fitness(population))]\n\n# Use the selected features for training the final model\nselected_features = np.where(best_individual == 1)[0]\nX_train_final, X_test_final = X_train[:, selected_features], X_test[:, selected_features]\n\n# Try a Random Forest as the final model\nfinal_model = RandomForestClassifier(random_state=42)\nfinal_model.fit(X_train_final, y_train)\n\n# Evaluate the final model using cross-validation\ncv_accuracy = np.mean(cross_val_score(final_model, X_test_final, y_test, cv=5))\nprint(\"Cross-Validation Accuracy:\", cv_accuracy)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''pip install transformers torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import torch\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm  # Import tqdm for the progress bar\n\n# Load the dataset\n# Assume df is your dataframe with columns 'Tweet' and 'Sentiment'\nX = df['Tweet'].values\ny = df['Sentiment'].values\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Load pre-trained BERT model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n# Tokenize and preprocess the data\ndef preprocess_data(texts, labels):\n    input_ids = []\n    attention_masks = []\n\n    for text in tqdm(texts, desc=\"Tokenizing and Preprocessing\"):\n        encoded_dict = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels)\n\n    return TensorDataset(input_ids, attention_masks, labels)\n\n# Preprocess the training and testing data\ntrain_dataset = preprocess_data(X_train, y_train)\ntest_dataset = preprocess_data(X_test, y_test)\n\n# Set up DataLoader\nbatch_size = 8\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Set up optimizer and training parameters\noptimizer = AdamW(model.parameters(), lr=2e-5)\nepochs = 3\n\n# Training loop\nfor epoch in range(epochs):\n    model.train()\n\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n        optimizer.zero_grad()\n\n        input_ids, attention_mask, labels = batch\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n# Evaluation\nmodel.eval()\npredictions = []\n\nfor batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n    input_ids, attention_mask, labels = batch\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n    logits = outputs.logits\n    predictions.extend(torch.argmax(logits, dim=1).tolist())\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, predictions)\nprint(\"BERT Accuracy:\", accuracy)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}